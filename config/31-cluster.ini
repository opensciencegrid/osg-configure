;===================================================================
;                          Subclusters
;===================================================================

; For each subcluster, add a new subcluster section.
; Each subcluster section must start with the words "Subcluster", and cannot be
; named "CHANGEME".

; There should be one subcluster section per set of homogeneous nodes in the
; cluster.

; This data is used to determine where OSG pilot jobs can be sent, so it's
; important to keep it up to date.

; If you have many similar subclusters, then feel free to collapse them into
; larger, approximately-correct groups.

; See example below:

;[Subcluster CHANGEME]
; should be the name of the subcluster
;name = SUBCLUSTER_NAME
; Megabytes of RAM per node.
;ram_mb = MB_OF_RAM
; Number of cores per node.
;cores_per_node = #_CORES_PER_NODE
; A list of VOs that are allowed to submit to this subcluster;
; If *, uses VOs that have accounts on this CE
;allowed_vos = vo1, vo2

; Non-mandatory attributes
; The maximum wall-clock time a job is allowed to run on this subcluster,
; in minutes.  Leave blank or set to 0 to indicate no wall time limit on jobs.
;max_wall_time = 1440
; The queue which jobs should be submitted to in order to run on this resource.
; Equivalent to the HTCondor grid universe classad attribute "remote_queue"
;queue = blue
; Transformation attributes which the HTCondor job router should apply to
; incoming jobs so they can run on this resource, as per
; http://research.cs.wisc.edu/htcondor/manual/v8.3/5_4HTCondor_Job.html
; These are in HTCondor classad syntax, and should be used sparingly.
;extra_transforms = [ set_WantRHEL6 = 1; ]

; Here's a full example.  Remember, globally unique names!
; [Subcluster Dell Nodes UNL]
; name = Dell Nodes UNL
; ram_mb = 4110
; cores_per_node = 4
; allowed_vos = osg, cms
; max_wall_time = 1440




;===================================================================
;                         Resource Entry
;===================================================================
; This section is for defining entry points into your clusters that
; will be advertised to the AGIS/CRIC system. Non-ATLAS CEs will
; not require this section.
;
;[Resource Entry CHANGEME]
;
;  Required attributes:
;
; The name of the resource - must be globally unique across the grid
;name = NAME_OF_RESOURCE
;
; The number of CPUs a job using this resource can get
;cpucount = CPUS_ALLOCATED_TO_JOB
;
; The max amount of memory (in MB) a job using this resource can get
;maxmemory = MAX_MB_OF_RAM_ALLOCATED_TO_JOB
;
; The maximum runtime (in minutes) a job using this resource can get
;max_wall_time = MAX_MINUTES_OF_RUNTIME
;
; The CE queue that jobs using this resource will go into
;queue = CHANGEME
;
; The list of VOs allowed to run jobs on this resource;
; if *, uses VOs that have accounts on this CE
;allowed_vos = vo1, vo2
;
;  Optional attributes:
;
; The physical subclusters this resource entry refers to.
; Subclusters with matching 'name' attributes must be defined
; elsewhere in this file.
;subclusters = CHANGEME 1, CHANGEME 2
;
; A free-form label to be added to jobs routed through this resource
;vo_tag = CHANGEME
