;===================================================================
;                          Pilots
;===================================================================

; For each pilot type, add a new pilot section.
; If you accept multiple pilot types, each section must have a different name.
; Names should only contain lowercase letters, numbers, "-" or "_", and should
; describe the capabilities of that type of pilot.

; Good names are "singularity_8core", "gpu", "bigmem", "main".

; The name will be used as-is as the "Name" attribute in the
; OSG_ResourceCatalog entry.

; This data is used to determine the resources requested by pilot jobs submitted by the OSG, so it's
; important to keep it up to date.


;[Pilot PILOT_TYPE]
;; The number of cores for this pilot type.
;cpucount = 1
;; The amount of memory (in megabytes) for this pilot type.
;ram_mb = 2500
;; This is a whole node pilot; cpucount and ram_mb are ignored if this is true.
;whole_node = false
;; The number of GPUs available
;gpucount = 0
;; The maximum number of pilots of this type that can be sent
;max_pilots = CHANGEME
;; The maximum wall-clock time a job is allowed to run for this pilot type,
;; in minutes.
;max_wall_time = 1440
;; The queue or partition which jobs should be submitted to in order to run on this resource.
;; Equivalent to the HTCondor grid universe classad attribute "remote_queue"
;queue =
;; True if the pilot should require singularity on the workers.
;require_singularity = true
;; The OS of the workers; allowed values are "rhel6", "rhel7", "rhel8", or "ubuntu18".
;; This is required unless require_singularity = true
;os = rhel7
;; Send test pilots?
;send_tests = true
;; A comma-separated list of VOs that are allowed to submit to this subcluster;
;; If *, uses VOs that have accounts on this CE
;allowed_vos = vo1, vo2
