;===================================================================
;                       IMPORTANT
;===================================================================
;
; 
; You can get documentation on the syntax of this file at:
; https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ConfigurationFileFormat
; You can get documentation on the options for each section at:
; https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ConfigurationFileHelp
;


[DEFAULT]
; Use this section to define variables that will be used in other sections
; For example, if you define a variable called dcache_root here
; you can use it in the gip section as %(dcache_root)s  (e.g. 
; my_vo_1_dir = %(dcache_root)s/my_vo_1
; my_vo_2_dir = %(dcache_root)s/my_vo_2

; Defaults, please don't modify these variables
unavailable = UNAVAILABLE
default = UNAVAILABLE

; Name these variables disable and enable rather than disabled and enabled
; to avoid infinite recursions
disable = False
enable = True

; You can modify the following and use them
localhost = my.host.name
admin_email = foo@my.domain
osg_location = UNAVAILABLE

;===================================================================
;                       Site Information
;===================================================================

[Site Information]
; The group option indicates the group that the OSG site should be listed in,
; for production sites this should be OSG, for vtb or itb testing it should be
; OSG-ITB
; 
; YOU WILL NEED TO CHANGE THIS
group = OSG-ITB

; The host_name setting should give the host name of the CE  that is being 
; configured, this setting must be a valid dns name that resolves
; 
; YOU WILL NEED TO CHANGE THIS
host_name = %(localhost)s

; The resource setting should be set to the same value as used in the OIM 
; registration at the goc 
; 
; YOU WILL NEED TO CHANGE THIS
resource = %(unavailable)s


; The resource_group setting should be set to the same value as used in the OIM 
; registration at the goc 
; 
; YOU WILL NEED TO CHANGE THIS
resource_group = %(unavailable)s

; The sponsor setting should list the sponsors for your cluster, if your cluster
; has multiple sponsors, you can separate them using commas or specify the 
; percentage using the following format 'osg, atlas, cms' or 
; 'osg:10, atlas:45, cms:45'  
; 
; YOU WILL NEED TO CHANGE THIS
sponsor = %(unavailable)s

; The site_policy setting should give an url that lists your site's usage
; policy 
site_policy = %(unavailable)s

; The contact setting should give the name of the admin/technical contact
; for the cluster
; 
; YOU WILL NEED TO CHANGE THIS
contact = %(unavailable)s

; The email setting should give the email address for the technical contact
; for the cluster 
; 
; YOU WILL NEED TO CHANGE THIS
email = %(admin_email)s

; The city setting should give the city that the cluster is located in
; 
; YOU WILL NEED TO CHANGE THIS
city = %(unavailable)s

; The country setting should give the country that the cluster is located in
; 
; YOU WILL NEED TO CHANGE THIS
country = %(unavailable)s

; The longitude setting should give the longitude for the cluster's location
; if you are in the US, this should be negative
; accepted values are between -180 and 180
; 
; YOU WILL NEED TO CHANGE THIS
longitude = %(unavailable)s 

; The latitude setting should give the latitude for the cluster's location
; accepted values are between -90 and 90
; 
; YOU WILL NEED TO CHANGE THIS
latitude = %(unavailable)s


;===================================================================
; For the following job manager sections (LSF, SGE, PBS, Condor)
; you should delete the sections corresponding to job managers that 
; you are not using.  E.g. if you are just using Condor on your
; site, you can delete the LSF, SGE and PBS sections.
;===================================================================


;===================================================================
;                              PBS
;===================================================================


[PBS]
; This section has settings for configuring your CE for a PBS job manager

; The enabled setting indicates whether you want your CE to use a PBS job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the pbs install directory
home = %(unavailable)s

; The pbs_location setting should give the location of pbs install directory
; This should be the same as the home setting above
pbs_location = %(home)s

; The job_contact setting should give the contact string for the jobmanager 
; on this CE (e.g. host.name/jobmanager-pbs) 
job_contact = %(localhost)s/jobmanager-pbs

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                             Condor
;===================================================================


[Condor]
; This section has settings for configuring your CE for a Condor job manager

; The enabled setting indicates whether you want your CE to use a Condor job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The condor_location setting should give the location of condor install directory
condor_location = %(unavailable)s

; The condor_location setting should give the location of condor config file,
; This is typically  etc/condor_config within the condor install directory.
; If you leave this set to %(unavailable)s, configure-osg will attempt to
; determine the correct value.
condor_config = %(unavailable)s

; The job_contact setting should give the contact string for the jobmanager 
; on this CE (e.g. host.name/jobmanager-condor) 
job_contact = %(localhost)s/jobmanager-condor

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              SGE
;===================================================================


[SGE]
; This section has settings for configuring your CE for a SGE job manager

; The enabled setting indicates whether you want your CE to use a SGE job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The sge_root setting should give the location of sge install directory
;
; The VDT will bootstrap your SGE environment by sourcing
;   $SGE_ROOT/$SGE_CELL/common/settings.sh
; where $SGE_ROOT and $SGE_CELL are the values given for sge_root and sge_cell.
sge_root = %(unavailable)s

; The sge_cell setting should be set to the value of $SGE_CELL for your SGE
; install.
sge_cell = %(unavailable)s


; The job_contact setting should give the contact string for the jobmanager 
; on this CE (e.g. host.name/jobmanager-sge) 
job_contact = %(localhost)s/jobmanager-sge

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              LSF
;===================================================================


[LSF]
; This section has settings for configuring your CE for a LSF job manager

; The enabled setting indicates whether you want your CE to use a LSF job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the lsf install directory
home = %(unavailable)s

; The lsf_location setting should give the location of lsf install directory
; This should be the same as the home setting above
lsf_location = %(home)s

; The job_contact setting should give the contact string for the jobmanager 
; on this CE (e.g. host.name/jobmanager-lsf) 
job_contact = %(localhost)s/jobmanager-lsf

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              Managed Fork
;===================================================================


[Managed Fork]
; The enabled setting indicates whether managed fork is in use on the system
; or not. You should set this to True or False
enabled = %(disable)s


;===================================================================
;                              Misc Services
;===================================================================


[Misc Services]
; If you have glexec installed on your worker nodes, enter the location
; of the glexec binary in this setting
glexec_location = %(unavailable)s

; If you wish to use the ca certificate update service, set this setting to True, 
; otherwise keep this at false
; Please note that as of OSG 1.0, you have to use the ca cert updater or the RPM
; updates, pacman can not update the ca certs
use_cert_updater = %(disable)s

; This setting should be set to the host used for gums host.  
; If your site is not using a gums host, you can set this to %(unavailable)s
gums_host = %(unavailable)s

; This setting should be set to one of the following: gridmap, prima, xacml 
; to indicate whether gridmap files, prima callouts, or prima callouts with xacml
; should be used
authorization_method = %(unavailable)s

; This setting indicates whether the osg index page generation will be run,
; by default this is not run
enable_webpage_creation = %(disable)s

;===================================================================
;                         CEMon
;===================================================================

[Cemon]

; Default servers for production and itb OSG ress/bdii servers, please don't touch

; The current production osg servers
; Production ReSS server
osg-ress-servers = https://osg-ress-1.fnal.gov:8443/ig/services/CEInfoCollector[OLD_CLASSAD]
; Production BDII server
osg-bdii-servers = http://is1.grid.iu.edu:14001[RAW], http://is2.grid.iu.edu:14001[RAW]

; The current itb osg servers
; ITB ReSS server
itb-ress-servers = https://osg-ress-4.fnal.gov:8443/ig/services/CEInfoCollector[OLD_CLASSAD]
; ITB BDII server
itb-bdii-servers = http://is-itb1.grid.iu.edu:14001[RAW], http://is-itb2.grid.iu.edu:14001[RAW]
 

; The enable option indicates whether cemon should be enabled or 
; disabled.  It should be set to True or False
;
; You generally want Cemon enabled for any CE installation
enabled = %(enable)s

; This setting indicates which servers ress information should
; be sent to.  Most sites should use the %(osg-ress-servers)s 
; setting so that the predefined variable giving the default
; osg production servers will be used, ITB admins can use 
; %(itb-ress-servers)s for default itb servers
;
; The server list should be formated as follows:
; server_uri[format],server_uri[format]
; see the variables at the top of this section for examples of this 
ress_servers = %(default)s

; This setting indicates which servers bdii information should
; be sent to.  Most sites should use the %(osg-bdii-servers)s 
; setting so that the predefined variable giving the default
; osg production servers will be used, ITB admins can use 
; %(itb-bdii-servers)s for default itb servers
; 
; The formatting for this are the same as the ress_servers setting 
bdii_servers = %(default)s

;===================================================================
;                         Gratia
;===================================================================

[Gratia]

; Default gratia servers
; 
; Please don't change these unless you have good reason to do so

; Variables for osg itb probes, CE installations should use the 
; jobmanager and metric probes 
itb-jobmanager-gratia = jobmanager:gratia-osg-itb.opensciencegrid.org:80
itb-gridftp-gratia = gridftp:gratia-osg-itb.opensciencegrid.org:80
itb-metric-gratia = metric:rsv-itb.grid.iu.edu:8880

; Variables for osg production probes, CE installations should use the 
; jobmanager and metric probes 
osg-jobmanager-gratia = jobmanager:gratia-osg-prod.opensciencegrid.org:80
osg-gridftp-gratia = gridftp:gratia-osg-transfer.opensciencegrid.org:80
osg-metric-gratia = metric:rsv.grid.iu.edu:8880
    

; The enable option indicates whether gratia should be enabled or 
; disabled.  It should be set to True or False
;
; You generally want Gratia enabled for any CE installation, in 
; addition SE installations may want to enable gratia to use 
; the gratia gridftp reporting
enabled = %(enable)s


; This setting specifies the resource that gratia will use to report
; accounting information, on a CE if you leave this blank, gratia will
; use the resource setting from the Site Information section
;
resource = %(unavailable)s

; This setting indicates which probes should be enabled for gratia
; The list should be given as probe_name1:host1:port1, probe_name2:host2:port2 
; where probe_name is either jobmanager, metric, or gridftp
; host is a fully qualified domain name
; port is the port that the server is listening on
; CEs should have entries for jobmanager and metric probes
; SEs should use gridftp if they would like to enable gridftp transfer
; accounting 
; for convience admins can use %(osg-jobmanager-gratia)s,
; %(osg-gridftp-gratia)s, %(osg-metric-gratia)s for production sites
; and %(itb-jobmanager-gratia)s, %(itb-gridftp-gratia)s, 
; and %(itb-metric-gratia)s for ITB sites
probes = %(default)s


;===================================================================
;                            RSV
;===================================================================


[RSV]
; The enable option indicates whether rsv should be enable or disabled.  It should
; be set to True or False
enabled = %(disable)s

; The rsv_user option gives the user that the rsv service should use.  It must
; be a valid unix user account
; 
; If rsv is enabled, and this is blank or set to unavailable it will default to 
; rsvuser
rsv_user = %(default)s

; This setting specifies which rsv gratia probes should be used, set this to 
; UNAVAILABLE to disable gratia probes, otherwise give a list of probes separated by
; commas. Valid probes include metric, condor, pbs, lsf, sge, 
; hadoop-transfer, and gridftp-transfer
;
; On a CE, you will probably want the metric probe (for RSV), the probe for your 
; jobmanager and managedfork if you are using that
;
; For example, on a CE using pbs and the managed fork, you'll probably use
; gratia_probes = metric, pbs, condor
gratia_probes = %(unavailable)s

; The enable_ce_probes option enables or disables the RSV CE probes.  If you enable this,
; you should also set the ce_hosts option as well.
;
; Set this to true or false. 
enable_ce_probes = %(disable)s

; The ce_hosts options lists the FQDN of the CEs that the RSV CE probes should check. 
; This should be a list of FQDNs separated by a comma (e.g. my.host,my.host2,my.host3)
;
; This must be set if the enable_ce_probes option is enabled.  If this is set to 
; UNAVAILABLE or left blank, then it will default to the hostname setting for this CE
ce_hosts = %(default)s

; The enable_gridftp_probes option enables or disables the RSV gridftp probes.  If 
; you enable this, you must also set the ce_hosts or gridftp_hosts option as well.
;
; Set this to True or False. 
enable_gridftp_probes = %(disable)s

; The gridftp_hosts options lists the FQDN of the gridftp servers that the RSV CE 
; probes should check. This should be a list of FQDNs separated by a comma 
; (e.g. my.host,my.host2,my.host3)
;
; This or ce_hosts must be set if the enable_gridftp_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
gridftp_hosts = %(default)s

; The gridftp_dir options gives the directory  on the gridftp servers that the 
; RSV CE  probes should try to write and read from. 
;
; This should be set if the enable_gridftp_probes option is enabled. It will default
; to /tmp if left blank or set to UNAVAILABLE 
gridftp_dir = %(default)s

; The enable_gums_probes option enables or disables the RSV gums probes.  If 
; you enable this, you must also set the ce_hosts or gums_hosts option as well.
;
; Set this to True or False. 
enable_gums_probes = %(disable)s

; The gums_hosts options lists the FQDN of the CE that uses GUMS server that the 
; RSV GUMS probes should check. This should be a list of FQDNs separated by a 
; comma (e.g. my.host,my.host2,my.host3)
;
; This or ce_hosts should be set if the enable_gums_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
gums_hosts = %(default)s

; The enable_srm_probes option enables or disables the RSV srm probes.  If 
; you enable this, you must also set the srm_hosts option as well.
;
; Set this to True or False. 
enable_srm_probes = %(disable)s

; The srm_hosts options lists the FQDN of the srm servers that the 
; RSV SRM probes should check. This should be a list of FQDNs separated 
; by a comma (e.g. my.host,my.host2,my.host3).  You can specify the port 
; on a host using host:port (e.g. localhost:8443 ). 
;
; This or _hosts must be set if the enable_srm_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
srm_hosts = %(default)s

; The srm_dir options gives the directory  on the srm servers that the 
; RSV SRM probes should try to write and read from. 
;
; This must be set if the enable_srm_probes option is enabled. 
srm_dir = %(unavailable)s

; This option gives the webservice path that SRM probes need to along with the 
; host: port. For dcache installations, this should work if left blank or left out. 
; However Bestman-xrootd SEs normally use srm/v2/server as web service path, and so 
; Bestman-xrootd admins will have to pass this option with the appropriate value 
; (for example: "srm/v2/server") for the SRM probes to work on their SE.
srm_webservice_path = %(unavailable)s

; Use the use_service_cert option indicates whether to use a service 
; certificate with rsv 
;
; NOTE: This can't be used if you specify multiple CEs or GUMS hosts
use_service_cert = %(disable)s

; You'll need to set this if you have enabled the use_service_cert.  
; This should point to the public key file (pem) for your service 
; certificate
; 
; If this is left blank or set to UNAVAILABLE  and the use_service_cert 
; setting is enabled, it will default to /etc/grid-security/rsvcert.pem
rsv_cert_file  = %(default)s

; You'll need to set this if you have enabled the use_service_cert.  
; This should point to the private key file (pem) for your service 
; certificate
;
; If this is left blank or set to UNAVAILABLE and the use_service_cert 
; setting is enabled, it will default to /etc/grid-security/rsvkey.pem
rsv_key_file  = %(default)s

; You'll need to set this if you have enabled the use_service_cert.  This 
; should point to the location of the rsv proxy file.
;
; If this is left blank or set to UNAVAILABLE and the use_service_cert 
; setting is enabled, it will default to /tmp/rsvproxy
rsv_proxy_out_file = %(default)s

; If you don't use a service certificate for rsv, you will need to specify a 
; proxy file that RSV should use in the proxy_file setting.
; This needs to be set if use_service_cert is disabled
proxy_file = %(unavailable)s

; This option will enable RSV record uploading to central RSV collector at the GOC
;
; Set this to True or False
enable_gratia = %(disable)s

; The print_local_time option indicates whether rsv should use local times instead of 
; GMT times in the local web pages produced (NOTE: records uploaded to central RSV 
; collector will still have UTC timestamps)
;
; Set this to True or False
print_local_time = %(disable)s

; The setup_rsv_nagios option indicates whether rsv try to connect to a locat
; nagios instance and report information to it as well
;
; Set this to True or False
setup_rsv_nagios = %(disable)s

; The rsv_nagios_conf_file option indicates the location of the rsv nagios 
; file to use for configuration details.  This is optional.  If this is set
; to %(default)s or %(unavailable)s then a default value that should work
; will be used by configure-osg.
;
rsv_nagios_conf_file = %(default)s

; The setup_for_apache option indicates whether rsv try to create a webpage
; that can be used to view the status of the rsv tests.  Enabling this is 
; highly encouraged.
;
; Set this to True or False
setup_for_apache = %(disable)s


;===================================================================
;                            Storage 
;===================================================================

[Storage]
;
; Several of these values are constrained and need to be set in a way
; that is consistent with one of the OSG storage models
;
; Please refer to the OSG release documentation for an indepth explanation 
; of the various storage models and the requirements for them

; If you have a SE available for your cluster and wish to make it available 
; to incoming jobs, set se_available to True, otherwise set it to False
se_available = %(disable)s

; If you indicated that you have an se available at your cluster, set default_se to
; the hostname of this SE, otherwise set default_se to UNAVAILABLE
default_se = %(unavailable)s

; The grid_dir setting should point to the directory which holds the files 
; from the OSG worker node package, it should be visible on all of the computer
; nodes (read access is required, worker nodes don't need to be able to write) 
; 
; YOU WILL NEED TO CHANGE THIS
grid_dir = %(unavailable)s

; The app_dir setting should point to the directory which contains the VO 
; specific applications, this should be visible on both the CE and worker nodes
; but only the CE needs to have write access to this directory
; 
; YOU WILL NEED TO CHANGE THIS
app_dir = %(unavailable)s

; The data_dir setting should point to a directory that can be used to store 
; and stage data in and out of the cluster.  This directory should be readable
; and writable on both the CE and worker nodes
; 
; YOU WILL NEED TO CHANGE THIS
data_dir = %(unavailable)s

; The worker_node_temp directory should point to a directory that can be used 
; as scratch space on compute nodes, it should allow read and write access on the 
; worker nodes but can be local to each worker node
; 
; YOU WILL NEED TO CHANGE THIS
worker_node_temp = %(unavailable)s

; The site_read setting should be the location or url to a directory that can 
; be read to stage in data, this is an url if you are using a SE 
; 
; YOU WILL NEED TO CHANGE THIS
site_read = %(unavailable)s

; The site_write setting should be the location or url to a directory that can 
; be write to stage out data, this is an url if you are using a SE 
; 
; YOU WILL NEED TO CHANGE THIS
site_write = %(unavailable)s


;===================================================================
;                             Squid
;===================================================================

[Squid]
; Set the enabled setting to True if you have squid installed and wish to 
; use it, otherwise set it to False 
enabled = %(disable)s

; If you are using squid, specify the location of the squid server in the 
; location setting, this can be a path if squid is installed on the same
; server as the CE or it can be a hostname
location = %(unavailable)s

; If you are using squid, use the policy setting to indicate which cache
; replacement policy squid is using
policy = %(unavailable)s

; If you are using squid, use the cache_size setting to indicate which the 
; size of the disk cache that squid is using
cache_size = %(unavailable)s

; If you are using squid, use the memory_size setting to indicate which the 
; size of the memory cache that squid is using
memory_size = %(unavailable)s


;===================================================================
;                              GIP
;===================================================================

[GIP]

; ========= These settings must be changed ==============

;; This setting indicates the batch system that GIP should query
;; and advertise
;; This should be the name of the batch system in lowercase
batch = %(unavailable)s
;; Options include: pbs, lsf, sge, or condor

; ========= These settings can be left as is for the standard install ========

;; This setting indicates whether GIP should advertise a gsiftp server
;; in addition to a srm server, if you don't have a srm server, this should
;; be enabled
;; Valid options are True or False
advertise_gsiftp = %(enable)s

;; This should be the hostname of the gsiftp server that gip will advertise
gsiftp_host = %(localhost)s

;; This setting indicates whether GIP should query the gums server.
;; Valid options are True or False
advertise_gums = %(disable)s

;
; NOTE ABOUT PREVIOUS GIP OPTIONS:
; There used to be many more options in the GIP section, mostly involving the
; configuration of the site's subclusters and storage elements.  These options
; have been moved into separate sections - one section per subcluster or SE.
; However, backward compatibility with the older format has been retained, and
;

;===================================================================
;                          Subclusters
;===================================================================

; For each subcluster, add a new subcluster section.
; Each subcluster name must be unique for the entire grid, so make sure to not
; pick anything generic like "MAIN".  Each subcluster section must start with
; the words "Subcluster", and cannot be named "CHANGEME".

; There should be one subcluster section per set of homogeneous nodes in the
; cluster.

; This data is used for our statistics collections in the OSG, so it's important
; to keep it up to date.  This is important for WLCG sites as it will be used
; to determine your progress toward your MoU commitments!

; If you have many similar subclusters, then feel free to collapse them into
; larger, approximately-correct groups.

; See example below:

[Subcluster CHANGEME]
; should be the name of the subcluster
name = SUBCLUSTER_NAME
; number of homogeneous nodes in the subcluster
node_count = NUMBER_OF_NODE
; Megabytes of RAM per node.
ram_mb = MB_OF_RAM
; CPU model, as taken from /proc/cpuinfo.  Please, no abbreviations!
cpu_model = CPU_MODEL_FROM_/proc/cpuinfo
; Should be something like:
; cpu_model = Dual-Core AMD Opteron(tm) Processor 2216
; Vendor's name -- AMD or Intel?
cpu_vendor = VENDOR_AMD_OR_INTEL
; Approximate speed, in MHZ, of the chips
cpu_speed_mhz = CLOCK_SPEED_MHZ
; Must be an integer.  Example: cpu_speed_mhz = 2400
; Platform; x86_64 or i686
cpu_platform = x86_64_OR_i686

; Number of CPUs (physical chips) per node
cpus_per_node = #_PHYSICAL_CHIPS_PER_NODE
; Number of cores per node.
cores_per_node = #_CORES_PER_NODE
; For a dual-socket quad-core, you would put cpus_per_node=2 and
; cores_per_node=8

; Set to true or false depending on inbound connectivity.  That is, external
; hosts can contact the worker nodes in this subcluster based on their hostname.
inbound_network = FALSE
; Set to true or false depending on outbound connectivity.  Set to true if the
; worker nodes in this subcluster can communicate with the external internet.
outbound_network = TRUE

; Non-mandatory attributes
; The amount of swap per host in MB
;  swap_mb = 4000
; The per-core SpecInt 2000 score.  This is usually computed for you.
;  SI00 = 2000
; The per-core SpecFloat 2000 score.  This is usually computed for you
;  SF00 = 2000

; Mandatory for WLCG reporting
; The per-core HEPSPEC score.  See your VO representative for more information.
;  HEPSPEC = 8
; The conversion factor from HEPSPEC to SI2K is accepted to be 250.

; Here's a full example.  Remember, globally unique names!
; [Subcluster Dell Nodes UNL]
; name = Dell Nodes UNL
; node_count = 53
; ram_mb = 4110
; swap_mb = 4000
; cpu_model = Dual-Core AMD Opteron(tm) Processor 2216
; cpu_vendor = AMD
; cpu_speed_mhz = 2400
; cpus_per_node = 2
; cores_per_node = 4
; inbound_network = FALSE
; outbound_network = TRUE


;===================================================================
;                             SE
;===================================================================

; For each storage element, add a new SE section.
; Each SE name must be unique for the entire grid, so make sure to not
; pick anything generic like "MAIN".  Each SE section must start with
; the words "SE", and cannot be named "CHANGEME".

; There are two main configuration types; one for dCache, one for BestMan

; Don't forget to change the section name!  One section per SE at the site.
[SE CHANGEME]

; The first part of this section shows options which are mandatory for all SEs.
; dCache and BestMan-specific portions are shown afterward.

; Set to False to turn off this SE
enabled = True

; Name of the SE; set to be the same as the OIM registered name
name = SE_CHANGEME
; The endpoint of the SE.  It MUST have the hostname, port, and the server
; location (/srm/v2/server in this case).  It MUST NOT have the ?SFN= string.
srm_endpoint = httpg://srm.example.com:8443/srm/v2/server
; dCache endpoint template: httpg://srm.example.com:8443/srm/managerv2

; How to collect data; the most generic implementation is called "static"
provider_implementation = static
; WLCG sites with a SE *must* use bestman, dcache, or dcache19
; Implementation and version of your SRM SE; usually dcache or bestman
implementation = bestman
; Version refers to the SE version, not the SRM version.
version = 2.2.1.foo
; dCache example: version = 1.9.1
; Default paths for all of your VOs; VONAME is replaced with the VO's name.
default_path = /mnt/bestman/home/VONAME
; Set a specific path for VOs which don't use the default path.
; Comma-separated list of VO:PATH pairs.  Not required.
; vo_dirs=cms:/mnt/bestman/cms, dzero:/mnt/bestman2/atlas

; If your SE provides a POSIX-like mount on your worker nodes, uncomment
; the following line:
; mount_point = /,/
; POSIX-like file systems include Lustre, NFS, HDFS, xrootdfs
; dCache site should not uncomment the line
; The value of `mount_point` should be two paths; first, the path where the
; file system is mounted on the worker nodes, followed by the exported directory
; of the file system.  If you mount your file system on the worker nodes with
; the following command:
;    mount -t nfs nfs.example.com:/exported/dir /mnt/nfs
; then mount_point should look like this:
;    mount_point = /mnt/nfs,/exported/dir

; For BestMan-based SEs, uncomment and fill in the following.
;  provider_implementation = bestman
;  implementation = bestman

; Set to TRUE if the bestman provide can use 'df' on the directory referenced
; above to get the freespace information.  If set to false, it probably won't
; detect the correct info.
;  use_df = True

; For dCache-based SEs, uncomment and fill in the following
; How to collect data; set to 'dcache' for dcache 1.8 (additional config req'd
; for this case), 'dcache19' for dcache 1.9, or 'static' for default values.
; provider_implementation = dcache19
;  implementation = dcache
;  If you use the dcache provider, see 
; http://twiki.grid.iu.edu/bin/view/InformationServices/DcacheGip
; If you use the dcache19 provider, you must fill in the location of your
; dCache's information provider:
;  infoprovider_endpoint = http://dcache.example.com:2288/info
; SE implementation name; leave as 'dcache'

; Here are working configs for BestMan and dCache
; [SE dCache]
; name = T2_Nebraska_Storage
; srm_endpoint = httpg://srm.unl.edu:8443/srm/managerv2
; provider_implementation = static
; implementation = dcache
; version = 1.8.0-15p6
; default_path = /pnfs/unl.edu/data4/VONAME

; [SE Hadoop]
; name = T2_Nebraska_Hadoop
; srm_endpoint = httpg://dcache07.unl.edu:8443/srm/v2/server
; provider_implementation = bestman
; implementation = bestman
; version = 2.2.1.2.e1
; default_path = /user/VONAME

